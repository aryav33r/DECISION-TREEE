{"cells":[{"cell_type":"markdown","metadata":{"id":"_AH42Hc90zcA"},"source":["**Theoretical**"]},{"cell_type":"markdown","metadata":{"id":"ykc5B5Ac_uZE"},"source":["1. **What is a Decision Tree, and how does it work\t?**\n"]},{"cell_type":"markdown","metadata":{"id":"ZVfmJV_31flB"},"source":["ANSWER : A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n","It's a flowchart-like structure where each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents an outcome (or a class label).\n","The goal of a Decision Tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n","It is called a decision tree because, similar to a tree, it starts with a root and then branches off into different nodes, and at each node, we make a decision based on the features available. This continues until we arrive at the leaf node which is the output.\n","\n","\n","How does it work?\n","\n","Data Preparation: The algorithm starts with a dataset containing features and a target variable.\n","Feature Selection: It selects the best feature to split the data based on a metric like Gini impurity or information gain. The feature that best separates the data into different classes or values is chosen. This selection process is recursive, for each split, the best feature is selected again, and it goes on, until it reaches the leaf node.\n","Splitting: The data is split into subsets based on the selected feature's values.\n","Recursive Process: Steps 2 and 3 are repeated for each subset until a stopping criterion is met (e.g., all data points in a subset belong to the same class, or a maximum depth is reached).\n","Prediction: To make a prediction for a new data point, the algorithm traverses the tree from the root node down to a leaf node, following the decision rules at each node based on the data point's features. The value or class at the leaf node is the prediction.\n","Example: Imagine you want to predict whether a customer will buy a product based on their age and income. A decision tree might look like this:\n","\n","If age \u003c 30:\n","  If income \u003c 50k:\n","    Prediction: Not buy\n","  Else:\n","    Prediction: Buy\n","Else:\n","  If income \u003c 75k:\n","    Prediction: Not buy\n","  Else:\n","    Prediction: Buy\n","Use code with caution\n","In this example, the tree uses age and income as features to make a prediction about buying behavior.\n","\n","Advantages of Decision Trees\n","\n","Easy to understand and interpret.\n","Can handle both numerical and categorical data.\n","Requires little data preparation.\n","Non-parametric, meaning they don't make assumptions about the data distribution.\n","Disadvantages of Decision Trees\n","\n","Prone to overfitting, especially with complex trees.\n","Can be unstable, meaning small changes in the data can lead to large changes in the tree structure.\n","May not be the best choice for very large datasets."]},{"cell_type":"markdown","metadata":{"id":"OclHKG8v1num"},"source":["2. '**What are impurity measures in Decision Trees\t?**"]},{"cell_type":"markdown","metadata":{"id":"JF-I60hg1tbs"},"source":["ANSWER : Impurity measures in Decision Trees.\n","\n","Impurity Measures\n","\n","Impurity measures are used in decision trees to evaluate how well a feature separates the data into different classes or values. The goal is to choose the feature that results in the purest subsets, meaning subsets where the majority of data points belong to the same class or have similar values.\n","\n","Here are two common impurity measures used in Decision Trees:\n","\n","Gini Impurity\n","\n","The Gini impurity is a measure of the probability of misclassifying a randomly chosen element from a dataset if it were randomly labeled according to the class distribution in the dataset.\n","It ranges from 0 to 1, where 0 represents a pure node (all data points belong to the same class) and 1 represents the most impure node (data points are evenly distributed across classes).\n","The lower the Gini impurity, the better the feature is at separating the data.\n","Information Gain/Entropy\n","\n","Entropy is a measure of the uncertainty or randomness in a dataset.\n","Information gain is the reduction in entropy achieved by splitting the data based on a particular feature.\n","It ranges from 0 to 1, where 0 represents no uncertainty (all data points belong to the same class) and 1 represents maximum uncertainty (data points are evenly distributed across classes).\n","The higher the information gain, the better the feature is at separating the data. And it's calculated using Entropy.\n","How Impurity Measures are Used in Decision Trees\n","\n","Feature Selection: When building a decision tree, the algorithm evaluates the impurity of each feature at each node.\n","Splitting: The feature that results in the greatest reduction in impurity (or the highest information gain) is chosen as the splitting feature for that node.\n","Recursive Process: This process is repeated recursively for each subset of data until a stopping criterion is met.\n","Choosing Between Gini Impurity and Information Gain\n","\n","Both Gini impurity and information gain are widely used and often produce similar results.\n","Gini impurity is computationally simpler to calculate than information gain.\n","Information gain tends to be more sensitive to features with a large number of distinct values."]},{"cell_type":"markdown","metadata":{"id":"HrbB6xWE2CKy"},"source":["3. **What is the mathematical formula for Gini Impurity\t?**"]},{"cell_type":"markdown","metadata":{"id":"zvEMgsJs2Jad"},"source":["ANSWER : Gini Impurity Formula\n","\n","The Gini impurity for a given set of data with k classes is calculated as follows:\n","\n","Gini Impurity = 1 - Σ (pi)^2\n","Use code with caution\n","where:\n","\n","pi is the proportion of data points belonging to class i in the set.\n","Σ denotes the summation over all classes (from i = 1 to k).\n","Explanation\n","\n","Proportion (pi): For each class i, calculate the proportion of data points belonging to that class within the set.\n","Square and Sum: Square each proportion (pi) and sum the squared values for all classes.\n","Subtract from 1: Subtract the sum of squared proportions from 1 to obtain the Gini impurity.\n","Example\n","\n","Suppose you have a dataset with 10 data points:\n","\n","6 belong to class A\n","4 belong to class B\n","To calculate the Gini impurity:\n","\n","Proportion of class A (pA): 6/10 = 0.6\n","Proportion of class B (pB): 4/10 = 0.4\n","Gini Impurity: 1 - [(0.6)^2 + (0.4)^2] = 1 - [0.36 + 0.16] = 0.48\n","Therefore, the Gini impurity for this dataset is 0.48.\n","\n","Interpretation\n","\n","A Gini impurity of 0 indicates a perfectly pure set where all data points belong to the same class.\n","A Gini impurity closer to 1 indicates a more impure set where data points are evenly distributed across classes.\n","In decision tree algorithms, the Gini impurity is used to evaluate the quality of a split. The goal is to choose the split that minimizes the Gini impurity of the resulting subsets, leading to purer nodes and a more accurate classification."]},{"cell_type":"markdown","metadata":{"id":"uCuBdGgN2Q_M"},"source":["4. **What is the mathematical formula for Entropy\t?**"]},{"cell_type":"markdown","metadata":{"id":"C-cdrpL62Y47"},"source":["AMSWER : the mathematical formula for Entropy.\n","\n","Entropy Formula\n","\n","Entropy is a measure of the uncertainty or randomness in a dataset. In the context of decision trees, it quantifies the impurity of a node.\n","\n","The formula for entropy is as follows:\n","\n","Entropy = - Σ (pi * log2(pi))\n","Use code with caution\n","where:\n","\n","pi is the proportion of data points belonging to class i in the set.\n","Σ denotes the summation over all classes (from i = 1 to k).\n","log2 represents the logarithm base 2.\n","Explanation\n","\n","Proportion (pi): For each class i, calculate the proportion of data points belonging to that class within the set.\n","Logarithmic Calculation: Calculate the logarithm base 2 of each proportion (pi).\n","Multiplication and Summation: Multiply each proportion (pi) by its corresponding logarithmic value and sum the results for all classes.\n","Negation: Negate the final sum to obtain the entropy.\n","Example\n","\n","Suppose you have a dataset with 10 data points:\n","\n","6 belong to class A\n","4 belong to class B\n","To calculate the entropy:\n","\n","Proportion of class A (pA): 6/10 = 0.6\n","Proportion of class B (pB): 4/10 = 0.4\n","Entropy: - [(0.6 * log2(0.6)) + (0.4 * log2(0.4))] ≈ 0.97\n","Therefore, the entropy for this dataset is approximately 0.97.\n","\n","Interpretation\n","\n","Entropy ranges from 0 to 1.\n","An entropy of 0 indicates a perfectly pure set where all data points belong to the same class.\n","An entropy of 1 indicates maximum uncertainty, where data points are evenly distributed across classes.\n","In decision trees, entropy is used in conjunction with information gain to select the best feature for splitting a node. The feature that results in the highest information gain (i.e., the greatest reduction in entropy) is chosen."]},{"cell_type":"markdown","metadata":{"id":"8PbW5A2B3CkA"},"source":["5. **What is Information Gain, and how is it used in Decision Trees\t?**"]},{"cell_type":"markdown","metadata":{"id":"eJQRKc2c3QDq"},"source":["ANSWER : Information Gain and its role in Decision Trees.\n","\n","What is Information Gain?\n","\n","Information gain is a concept from information theory that quantifies the reduction in uncertainty about a target variable after observing a particular feature. In decision trees, it's used to measure how much a feature improves the purity of a node when used for splitting the data.\n","\n","How is Information Gain calculated?\n","\n","Information gain is calculated using the following formula:\n","\n","Information Gain = Entropy(parent) - [Weighted Average]Entropy(children)\n","Use code with caution\n","where:\n","\n","Entropy(parent) is the entropy of the parent node before the split.\n","Entropy(children) is the entropy of each child node after the split.\n","Weighted Average is the weighted average of the children's entropies, based on the proportion of data points in each child node.\n","How is Information Gain used in Decision Trees?\n","\n","Feature Selection: When building a decision tree, the algorithm evaluates the information gain for each feature at each node.\n","\n","Splitting: The feature with the highest information gain is selected as the splitting feature for that node. This is because it's considered the feature that best reduces uncertainty and improves the purity of the resulting subsets.\n","\n","Recursive Process: This process is repeated recursively for each subset of data until a stopping criterion is met (e.g., all data points in a subset belong to the same class, or a maximum depth is reached).\n","\n","Why is Information Gain important?\n","\n","Information gain is crucial in decision trees because it helps the algorithm make informed decisions about which features to use for splitting the data. By selecting features that maximize information gain, the decision tree can effectively partition the data into more homogeneous subsets, leading to better classification or regression performance.\n","\n","In summary, information gain is a key concept in decision trees that helps the algorithm select the most informative features for splitting the data, leading to a more accurate and efficient model."]},{"cell_type":"markdown","metadata":{"id":"0gvfjq003Y4p"},"source":["6. **What is the difference between Gini Impurity and Entropy\t?**"]},{"cell_type":"markdown","metadata":{"id":"bZaNkZTg3kzW"},"source":["ANSWER :  the difference between Gini Impurity and Entropy.\n","\n","Both Gini Impurity and Entropy are metrics used in decision trees to measure the impurity of a node. They help the algorithm decide which feature to use for splitting the data at each node. While they often lead to similar results, there are some key differences between them:\n","\n","Gini Impurity\n","\n","Formula: Gini Impurity = 1 - Σ (pi)^2, where pi is the proportion of data points belonging to class i in the set.\n","Range: 0 to 0.5 (for binary classification) or 0 to 1 (for multi-class classification).\n","Interpretation:\n","0 represents a pure node (all data points belong to the same class).\n","Values closer to 0.5 or 1 represent increasing impurity.\n","Computation: Generally faster to compute than entropy.\n","Focus: Emphasizes misclassification probability.\n","Entropy\n","\n","Formula: Entropy = - Σ (pi * log2(pi)), where pi is the proportion of data points belonging to class i in the set.\n","Range: 0 to 1.\n","Interpretation:\n","0 represents a pure node (all data points belong to the same class).\n","Values closer to 1 represent increasing uncertainty or randomness.\n","Computation: Can be slightly slower to compute than Gini impurity due to the logarithmic calculation.\n","Focus: Measures the uncertainty or randomness in the data.\n","Key Differences\n","\n","Range: Gini impurity has a smaller range compared to entropy, especially in binary classification cases.\n","Calculation: Gini impurity is computationally simpler and faster to calculate than entropy.\n","Focus: Gini impurity focuses on the probability of misclassification, while entropy measures the uncertainty or randomness in the data.\n","Sensitivity: Entropy can be more sensitive to features with a large number of distinct values.\n","In Practice\n","\n","Both Gini impurity and entropy are widely used in decision tree algorithms, and they often produce similar results. The choice between them often depends on personal preference or specific requirements of the task.\n","\n","If computational efficiency is a concern, Gini impurity might be preferred due to its faster calculation.\n","If the dataset has features with many distinct values, entropy might be a better choice due to its sensitivity."]},{"cell_type":"markdown","metadata":{"id":"TMl4oUyR3w8H"},"source":["7. **What is the mathematical explanation behind Decision Trees\t?**"]},{"cell_type":"markdown","metadata":{"id":"9zIWQngF4AQN"},"source":["ANSWER : the mathematical explanation behind Decision Trees.\n","\n","Mathematical Foundation\n","\n","Decision Trees are built upon the principles of:\n","\n","Recursive Partitioning: The dataset is recursively divided into subsets based on the values of features, aiming to create increasingly homogeneous subsets with respect to the target variable.\n","\n","Impurity Measures: Metrics like Gini Impurity or Entropy are used to quantify the impurity or uncertainty within a node. The goal is to minimize impurity as the tree grows.\n","\n","Information Gain: Information Gain measures the reduction in impurity achieved by splitting a node based on a particular feature. The feature that maximizes information gain is chosen for the split.\n","\n","Tree Structure: The resulting tree structure represents a series of decisions based on feature values, leading to predictions at the leaf nodes.\n","\n","Mathematical Formulation\n","\n","Let's break down the mathematical steps involved in building a Decision Tree:\n","\n","Start with the entire dataset (D) and the target variable (Y).\n","\n","For each feature (X): a. Calculate the impurity of the current node using Gini Impurity or Entropy. b. For each possible split point of the feature:\n","\n"," i. Split the data into two subsets (D1 and D2) based on the split point.\n"," ii. Calculate the weighted average impurity of the two subsets.\n"," iii. Calculate the Information Gain for the split:\n","     `Information Gain = Impurity(D) - Weighted Average Impurity(D1, D2)`\n","c. Select the split point that maximizes Information Gain for the feature.\n","\n","Choose the feature with the highest Information Gain as the splitting feature for the current node.\n","\n","Create two child nodes based on the selected split point.\n","\n","Recursively repeat steps 2-4 for each child node until a stopping criterion is met:\n","\n","All data points in a node belong to the same class.\n","A maximum depth is reached.\n","The number of data points in a node falls below a threshold.\n","Assign the majority class (for classification) or the average value (for regression) to each leaf node.\n","\n","Mathematical Intuition\n","\n","The mathematical essence of Decision Trees lies in finding the optimal splits that minimize impurity and maximize information gain. By recursively applying this principle, the tree learns a hierarchical structure of decision rules that effectively separates data points based on their features and predicts the target variable.\n","\n","Note: While this explanation provides a simplified overview, the actual implementation of Decision Tree algorithms may involve further considerations and optimizations, such as handling missing values, pruning the tree to avoid overfitting, and dealing with continuous features."]},{"cell_type":"markdown","metadata":{"id":"DFTArwjd4Dtq"},"source":["8. **What is Pre-Pruning in Decision Trees\t?**"]},{"cell_type":"markdown","metadata":{"id":"TLEG-UsB4MRB"},"source":["ANSWER :  Pre-Pruning in Decision Trees.\n","\n","Pre-Pruning\n","\n","Pre-pruning, also known as early stopping, is a technique used in Decision Trees to prevent overfitting by stopping the tree growth early before it becomes too complex. It involves setting constraints or limits on the tree's growth during the building process.\n","\n","How Pre-Pruning Works\n","\n","Pre-pruning techniques typically involve stopping the tree growth based on certain criteria. Some common pre-pruning strategies include:\n","\n","Maximum Depth: Limiting the maximum depth of the tree, which restricts the number of levels or decisions the tree can make.\n","\n","Minimum Samples Split: Setting a minimum number of samples required to split a node. If a node has fewer samples than the specified threshold, it will not be split further.\n","\n","Minimum Samples Leaf: Setting a minimum number of samples required to be present in a leaf node. If a split would result in a leaf node with fewer samples than the threshold, the split is not performed.\n","\n","Maximum Leaf Nodes: Limiting the total number of leaf nodes in the tree.\n","\n","Maximum Features: Restricting the number of features considered for splitting at each node.\n","\n","Benefits of Pre-Pruning\n","\n","Reduces Overfitting: By stopping the tree growth early, pre-pruning helps prevent the tree from becoming too complex and memorizing the training data, which can lead to overfitting.\n","\n","Improves Generalization: A simpler tree obtained through pre-pruning is more likely to generalize well to unseen data, improving the model's performance on new examples.\n","\n","Reduces Computational Cost: Pre-pruning can reduce the computational cost of building the tree by stopping the growth process earlier.\n","\n","Considerations for Pre-Pruning\n","\n","Choosing the Right Criteria: Selecting the appropriate pre-pruning criteria and their values is crucial to achieving the desired balance between tree complexity and generalization performance.\n","\n","Hyperparameter Tuning: Pre-pruning often involves hyperparameters that need to be tuned using techniques like cross-validation to find the optimal values for the specific dataset.\n","\n","Trade-off with Underfitting: While pre-pruning helps prevent overfitting, excessive pruning can lead to underfitting, where the tree is too simple to capture the underlying patterns in the data.\n","\n","In summary, pre-pruning is a valuable technique in Decision Trees that helps prevent overfitting, improves generalization, and reduces computational cost. By carefully choosing the pre-pruning criteria and tuning the hyperparameters, you can build more robust and accurate Decision Tree models."]},{"cell_type":"markdown","metadata":{"id":"y9KebL604c5A"},"source":["9.  **What is Post-Pruning in Decision Trees\t?**"]},{"cell_type":"markdown","metadata":{"id":"sb61gwYP4ky8"},"source":["ANSWER :  Post-Pruning in Decision Trees.\n","\n","Post-Pruning\n","\n","Post-pruning, also known as backward pruning, is a technique used in Decision Trees to reduce overfitting by removing or collapsing nodes after the tree has been fully grown. It involves evaluating the performance of the tree on a validation dataset and selectively removing branches that do not improve generalization.\n","\n","How Post-Pruning Works\n","\n","Grow the Tree: The Decision Tree is first grown to its full size without any restrictions.\n","\n","Evaluate Performance: The performance of the tree is evaluated on a separate validation dataset using a suitable metric (e.g., accuracy, precision, recall).\n","\n","Prune Branches: Starting from the bottom of the tree, the algorithm iteratively evaluates the impact of removing or collapsing each node on the validation performance. If removing a node or branch does not decrease the performance or even improves it, the pruning is performed.\n","\n","Stopping Criteria: The pruning process continues until further pruning starts to decrease the validation performance or until a desired level of complexity is achieved.\n","\n","Types of Post-Pruning\n","\n","There are different approaches to post-pruning, including:\n","\n","Reduced Error Pruning: This method replaces a subtree with a leaf node if it reduces the error on the validation set.\n","Cost Complexity Pruning: This approach uses a cost function that balances the size of the tree with its error rate to find the optimal pruned tree.\n","Minimum Error Pruning: This technique selects the subtree with the lowest error on the validation set.\n","Benefits of Post-Pruning\n","\n","Reduces Overfitting: Post-pruning effectively removes unnecessary branches that might have overfit the training data, leading to a more generalized model.\n","\n","Improves Generalization: By simplifying the tree structure, post-pruning enhances the model's ability to generalize to unseen data.\n","\n","Better Accuracy: Post-pruning can often lead to improved accuracy on the validation and test datasets compared to an unpruned tree.\n","\n","Considerations for Post-Pruning\n","\n","Validation Dataset: A separate validation dataset is crucial for evaluating the performance of the pruned tree and avoiding overfitting to the training data.\n","\n","Computational Cost: Post-pruning can be computationally more expensive than pre-pruning, as it involves building the full tree first and then evaluating different pruning strategies.\n","\n","Pruning Criteria: Choosing the appropriate pruning criteria and stopping criteria is important to achieve the desired balance between tree complexity and generalization performance.\n","\n","post-pruning is a powerful technique for improving the performance and generalization of Decision Trees. By carefully applying post-pruning strategies, you can create more robust and accurate models that perform well on unseen data."]},{"cell_type":"markdown","metadata":{"id":"YtMh-ekH4toG"},"source":["10. **What is the difference between Pre-Pruning and Post-Pruning\t?**"]},{"cell_type":"markdown","metadata":{"id":"8cmLn2ug4zHA"},"source":["ANSWER : the difference between Pre-Pruning and Post-Pruning in Decision Trees.\n","\n","Both pre-pruning and post-pruning are techniques used to address overfitting in Decision Trees by controlling the tree's complexity. However, they differ in when and how they prune the tree:\n","\n","Pre-Pruning\n","\n","When: Occurs during the tree construction process.\n","How: Stops the tree from growing beyond a certain point by setting limits or constraints on its growth.\n","Mechanism: Uses criteria like maximum depth, minimum samples split, minimum samples leaf, maximum leaf nodes, or maximum features to stop further splitting of nodes.\n","Advantages:\n","Reduces overfitting by preventing the tree from becoming too complex.\n","Improves generalization to unseen data.\n","Reduces computational cost by stopping growth early.\n","Disadvantages:\n","Risk of underfitting if pruning criteria are too strict.\n","Difficulty in choosing the optimal pruning criteria beforehand.\n","Post-Pruning\n","\n","When: Occurs after the tree has been fully grown.\n","How: Selectively removes or collapses nodes that do not improve generalization performance on a validation dataset.\n","Mechanism: Evaluates the impact of removing nodes on the validation performance and prunes branches that don't improve or even worsen it.\n","Advantages:\n","More effective in reducing overfitting compared to pre-pruning.\n","Better chance of finding the optimal tree structure.\n","Often leads to improved accuracy on unseen data.\n","Disadvantages:\n","Computationally more expensive as it requires building the full tree first.\n","Needs a separate validation dataset for evaluation.\n","Key Differences in a Table\n","\n","Feature\tPre-Pruning\tPost-Pruning\n","Timing\tDuring tree construction\tAfter tree construction\n","Approach\tStops tree growth early\tRemoves nodes from a fully grown tree\n","Complexity\tSimpler to implement\tMore complex\n","Computational Cost\tLower\tHigher\n","Overfitting Control\tLess effective\tMore effective\n","Generalization\tGood\tPotentially better\n","In Summary\n","\n","Pre-pruning is a proactive approach that prevents overfitting by limiting tree growth.\n","Post-pruning is a reactive approach that removes unnecessary complexity after the tree is built.\n","The choice between pre-pruning and post-pruning depends on the specific dataset, computational resources, and desired level of accuracy. In many cases, post-pruning is preferred due to its ability to achieve better generalization performance. However, pre-pruning can be a good option when computational efficiency is a concern."]},{"cell_type":"markdown","metadata":{"id":"GOJQ6X-F4_U6"},"source":["11. **What is a Decision Tree Regressor\t?**"]},{"cell_type":"markdown","metadata":{"id":"ob3ttSFT5FgW"},"source":["ANSWER :  Decision Tree Regressors.\n","\n","Decision Tree Regressor\n","\n","A Decision Tree Regressor is a type of Decision Tree algorithm used for solving regression problems, where the target variable is continuous rather than categorical. It works by partitioning the data into smaller subsets and predicting the average value of the target variable within each subset.\n","\n","How it Works\n","\n","Tree Construction: The Decision Tree Regressor builds a tree structure by recursively splitting the data based on features, similar to a Decision Tree Classifier. However, instead of predicting a class label, it predicts a continuous value.\n","\n","Prediction: To make a prediction for a new data point, the algorithm traverses the tree from the root node down to a leaf node, following the decision rules at each node based on the data point's features. The predicted value is the average value of the target variable for all data points in that leaf node.\n","\n","Splitting Criteria: The splitting criteria for a Decision Tree Regressor are typically based on minimizing the variance or mean squared error within each subset. The goal is to create subsets that have similar target variable values.\n","\n","Example\n","\n","Imagine you want to predict the price of a house based on its features like size, location, and number of bedrooms. A Decision Tree Regressor might create a tree structure like this:\n","\n","If size \u003e 1500 sq ft:\n","  If location is urban:\n","    Predicted price = $500,000\n","  Else:\n","    Predicted price = $400,000\n","Else:\n","  If number of bedrooms \u003e 3:\n","    Predicted price = $350,000\n","  Else:\n","    Predicted price = $300,000\n","Use code with caution\n","In this example, the tree uses features like size, location, and number of bedrooms to predict the price of a house.\n","\n","Advantages of Decision Tree Regressors\n","\n","Easy to Understand and Interpret: The tree structure is easy to visualize and understand, making it a transparent model.\n","\n","Handles Non-linear Relationships: Decision Tree Regressors can capture non-linear relationships between features and the target variable.\n","\n","No Feature Scaling Required: Unlike some other regression algorithms, Decision Tree Regressors do not require feature scaling.\n","\n","Disadvantages of Decision Tree Regressors\n","\n","Prone to Overfitting: Decision Tree Regressors can easily overfit the training data, leading to poor generalization performance. Techniques like pruning and ensemble methods can help mitigate this issue.\n","\n","Instability: Small changes in the data can lead to significant changes in the tree structure, making the model unstable.\n","\n","Limited Expressiveness: Decision Tree Regressors may not be able to capture complex relationships between features and the target variable as effectively as some other regression algorithms."]},{"cell_type":"markdown","metadata":{"id":"qatoe1S95V0Y"},"source":["12. **What are the advantages and disadvantages of Decision Trees\t?**"]},{"cell_type":"markdown","metadata":{"id":"64Hdaz1t5dnB"},"source":["ANSWER :  the advantages and disadvantages of Decision Trees.\n","\n","Advantages of Decision Trees\n","\n","Easy to Understand and Interpret: Decision Trees are easy to visualize and understand, making them transparent and interpretable models. The tree structure clearly shows the decision rules used for prediction, which can be easily explained to non-technical stakeholders.\n","\n","Handles Both Numerical and Categorical Data: Decision Trees can handle both numerical and categorical data without requiring extensive data preprocessing. This makes them versatile and applicable to a wide range of datasets.\n","\n","Requires Little Data Preparation: Decision Trees typically require minimal data preparation compared to other algorithms. They are less sensitive to outliers and missing values, and they don't require feature scaling or normalization.\n","\n","Non-parametric: Decision Trees are non-parametric models, meaning they don't make assumptions about the underlying data distribution. This makes them robust and applicable to datasets with complex or unknown distributions.\n","\n","Feature Importance: Decision Trees can provide insights into the importance of different features in making predictions. The features used for splitting at higher levels of the tree are generally considered more important.\n","\n","Disadvantages of Decision Trees\n","\n","Prone to Overfitting: Decision Trees can easily overfit the training data, especially when they are deep and complex. This can lead to poor generalization performance on unseen data. Techniques like pruning and ensemble methods (e.g., Random Forests) can help mitigate this issue.\n","\n","Instability: Decision Trees can be unstable, meaning small changes in the data can lead to significant changes in the tree structure. This can make the model less reliable and difficult to reproduce.\n","\n","Limited Expressiveness: Decision Trees may not be able to capture complex relationships between features and the target variable as effectively as some other algorithms, such as neural networks. They are better suited for datasets with relatively simple relationships.\n","\n","Bias Towards Dominant Features: Decision Trees can be biased towards features with more distinct values. This can lead to inaccurate predictions if the dataset has features with a large number of unique values.\n","\n","Computational Cost: While building a single decision tree is relatively fast, training complex ensembles of decision trees (e.g., Random Forests) can be computationally expensive.\n","\n","\n","\n","Decision Trees offer a good balance between interpretability, flexibility, and performance. They are widely used for various tasks, such as classification, regression, and feature selection. However, it's important to be aware of their limitations and apply appropriate techniques to mitigate overfitting and instability."]},{"cell_type":"markdown","metadata":{"id":"Wo_P8mX05z3q"},"source":["13. **How does a Decision Tree handle missing values?**"]},{"cell_type":"markdown","metadata":{"id":"ZaRoleFD5-L2"},"source":["ANSWER : Decision Trees handle missing values.\n","\n","Decision Trees have inherent mechanisms for handling missing values during both the training and prediction phases. Here's how they address this issue:\n","\n","During Training\n","\n","Ignoring Missing Values: In some implementations, the simplest approach is to ignore data points with missing values for the specific feature being considered for a split. This can lead to a loss of information, but it's computationally efficient.\n","\n","Imputation with the Most Frequent Value: For categorical features, missing values can be replaced with the most frequent value of that feature in the training data. This is a simple and common imputation technique.\n","\n","Imputation with the Mean/Median: For numerical features, missing values can be replaced with the mean or median value of that feature in the training data. This helps maintain the distribution of the feature.\n","\n","Surrogate Splits: This technique involves finding a surrogate feature that is highly correlated with the feature containing missing values. When a data point has a missing value for the primary feature, the surrogate feature is used to determine the split instead. This helps preserve information and improve accuracy.\n","\n","During Prediction\n","\n","Following the Branch with the Most Frequent Value: If a data point has a missing value for a feature used in a split, the prediction process follows the branch corresponding to the most frequent value of that feature in the training data.\n","\n","Following Multiple Branches with Probabilities: A more sophisticated approach involves following multiple branches with probabilities proportional to the frequency of each value in the training data. This provides a more nuanced prediction based on the distribution of the missing feature.\n","\n","Predicting the Average Value: In some cases, if a data point has missing values for multiple features used in a sequence of splits, the prediction might simply be the average value of the target variable for all data points that reach that specific leaf node.\n","\n","Advantages of Decision Tree Handling of Missing Values\n","\n","No Need for Explicit Imputation: Decision Trees can handle missing values directly without requiring explicit imputation before training.\n","\n","Robustness: Decision Trees are generally robust to missing values and can still provide accurate predictions even with incomplete data.\n","\n","Flexibility: Decision Trees offer various strategies for handling missing values, allowing you to choose the approach that best suits your data and task.\n","\n","\n","\n","Decision Trees offer efficient and effective ways to handle missing values. They can use various strategies during both training and prediction to minimize the impact of missing data on the model's performance. This robustness is one of the reasons why Decision Trees are a popular choice for various data science and machine learning applications."]},{"cell_type":"markdown","metadata":{"id":"QBPAPczZ6Xej"},"source":["14. **How does a Decision Tree handle categorical features\t?**"]},{"cell_type":"markdown","metadata":{"id":"pRXLedeb6hnG"},"source":["ANSWER : a Decision Tree handles categorical features.\n","\n","Decision Trees can handle categorical features effectively using various techniques. Here's a breakdown of the common approaches:\n","\n","1. Binary Splitting for Nominal Features:\n","\n","Nominal features are categorical features without any inherent order (e.g., colors, countries).\n","For nominal features with a small number of categories, the decision tree can create a separate branch for each category.\n","For nominal features with a large number of categories, binary splitting is often used.\n","In binary splitting, the categories are divided into two subsets, and the split is based on whether a data point belongs to one subset or the other.\n","The algorithm searches for the best binary split that maximizes information gain or minimizes impurity.\n","2. Ordinal Encoding for Ordinal Features:\n","\n","Ordinal features are categorical features with a natural order (e.g., education levels, customer satisfaction ratings).\n","Ordinal encoding assigns numerical values to the categories based on their order.\n","This allows the decision tree to treat the ordinal feature as a numerical feature and perform splits based on numerical thresholds.\n","3. One-Hot Encoding for Nominal Features (with Many Categories):\n","\n","One-hot encoding creates a new binary feature for each category of the nominal feature.\n","If a data point belongs to a particular category, the corresponding binary feature is set to 1, and all other binary features are set to 0.\n","This allows the decision tree to consider each category as a separate feature and make splits based on their presence or absence.\n","Example:\n","\n","Let's say you have a categorical feature \"Color\" with categories: Red, Green, and Blue.\n","\n","Binary Splitting: The tree might split the data into two subsets: {Red, Green} and {Blue}.\n","One-Hot Encoding: Three new binary features would be created: \"Color_Red,\" \"Color_Green,\" and \"Color_Blue.\" A data point with color \"Red\" would have \"Color_Red\" set to 1 and the other two features set to 0.\n","Choosing the Right Approach:\n","\n","The choice of which approach to use depends on the specific dataset and the nature of the categorical feature.\n","\n","Binary splitting is generally preferred for nominal features with a small number of categories.\n","Ordinal encoding is suitable for ordinal features where the order of categories is meaningful.\n","One-hot encoding is often used for nominal features with a large number of categories to avoid creating too many branches in the tree.\n","Advantages of Decision Tree Handling of Categorical Features:\n","\n","Flexibility: Decision Trees can handle various types of categorical features using different encoding schemes.\n","Interpretability: The splits based on categorical features are easy to understand and interpret.\n","No Assumptions about Data Distribution: Decision Trees don't make assumptions about the distribution of categorical features, making them robust to different data patterns.\n"," Decision Trees provide flexible and effective ways to handle categorical features, allowing them to be used in a wide range of data science and machine learning application ."]},{"cell_type":"markdown","metadata":{"id":"9eAguYDY6sZM"},"source":["15. ** What are some real-world applications of Decision Trees?**"]},{"cell_type":"markdown","metadata":{"id":"hW5e-Dyr6ycV"},"source":["ANSWER : some real-world applications of Decision Trees.\n","\n","Decision Trees are versatile algorithms used in various domains for solving both classification and regression problems. Here are some notable real-world applications:\n","\n","1. Customer Relationship Management (CRM):\n","\n","Customer Churn Prediction: Decision Trees can be used to identify customers who are likely to churn (cancel their subscription or stop using a service). By analyzing customer demographics, purchase history, and interactions, the tree can identify patterns and predict churn risk. This allows businesses to take proactive measures to retain valuable customers.\n","Targeted Marketing: Decision Trees can help segment customers based on their characteristics and preferences, enabling businesses to target specific groups with personalized marketing campaigns. This leads to more effective marketing efforts and improved customer engagement.\n","2. Healthcare:\n","\n","Disease Diagnosis: Decision Trees can assist in diagnosing diseases based on patient symptoms, medical history, and test results. By learning patterns from historical data, the tree can provide probabilities of different diseases and aid doctors in making informed decisions.\n","Treatment Recommendation: Decision Trees can be used to recommend personalized treatment plans based on patient characteristics and disease stage. By considering various factors, the tree can suggest the most effective treatment options, improving patient outcomes.\n","3. Finance:\n","\n","Loan Approval: Decision Trees can be used to assess the creditworthiness of loan applicants. By analyzing financial data such as income, debt, and credit history, the tree can predict the likelihood of loan default and help financial institutions make informed lending decisions.\n","Fraud Detection: Decision Trees can be employed to detect fraudulent transactions by identifying suspicious patterns in financial data. By learning from historical fraud cases, the tree can flag potentially fraudulent activities and prevent financial losses.\n","4. Operations and Manufacturing:\n","\n","Predictive Maintenance: Decision Trees can be used to predict equipment failures and schedule maintenance proactively. By analyzing sensor data and historical maintenance records, the tree can identify patterns that indicate impending failures, minimizing downtime and reducing costs.\n","Quality Control: Decision Trees can help identify defects in manufactured products by analyzing production data and quality inspection results. By learning from historical defect patterns, the tree can flag potential quality issues and improve product quality.\n","5. Natural Language Processing (NLP):\n","\n","Sentiment Analysis: Decision Trees can be used to classify text into different sentiment categories (e.g., positive, negative, neutral). By analyzing the words and phrases used in the text, the tree can determine the overall sentiment expressed.\n","Spam Filtering: Decision Trees can be employed to identify and filter spam emails by analyzing the content and sender information. By learning from known spam patterns, the tree can classify incoming emails as spam or legitimate.\n","These are just a few examples of the many real-world applications of Decision Trees. Their versatility and interpretability make them valuable tools for solving a wide range of problems across various industries."]},{"cell_type":"markdown","metadata":{"id":"IVNbq7tM64G1"},"source":["**Practical**"]},{"cell_type":"markdown","metadata":{"id":"GNuvev4gAI5Z"},"source":["16. **Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy ?**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4304,"status":"ok","timestamp":1747296848651,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"kjEhsm0cAXYZ","outputId":"4607c8d0-0166-4b69-dfc3-f3a1bb243201"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 1.0\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier object\n","clf = DecisionTreeClassifier()\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = clf.predict(X_test)\n","\n","# Calculate the accuracy of the model\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"ZM00Z3IJAhA-"},"source":["Reasoning:\n","\n","Import necessary libraries: We import load_iris to load the dataset, train_test_split to split data, DecisionTreeClassifier for the model, and accuracy_score for evaluation.\n","Load the Iris dataset: load_iris() loads the dataset, and we assign features to X and the target variable to y.\n","Split the data: train_test_split divides the data into training and testing sets (70% for training, 30% for testing). random_state ensures consistent splitting.\n","Create and train the model: We create a DecisionTreeClassifier object and train it using the training data (X_train, y_train).\n","Make predictions: We use the trained model to predict on the testing data (X_test).\n","Calculate and print accuracy: accuracy_score compares predictions (y_pred) with actual labels (y_test) to calculate accuracy, which is then printed."]},{"cell_type":"markdown","metadata":{"id":"xLHUujfFAh_R"},"source":["17. **Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n","feature importances ?**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1747296966988,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"WqRSOZaJAwc9","outputId":"2929f299-8a84-4504-a845-aaa47a029b12"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature 0: sepal length (cm), Importance: 0.01911001911001911\n","Feature 1: sepal width (cm), Importance: 0.01911001911001911\n","Feature 2: petal length (cm), Importance: 0.8741535326901181\n","Feature 3: petal width (cm), Importance: 0.08762642908984374\n"]}],"source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a DecisionTreeClassifier object with Gini impurity as the criterion\n","clf = DecisionTreeClassifier(criterion='gini') # default is gini\n","\n","# Fit the model to the training data\n","clf.fit(X_train, y_train)\n","\n","# Get feature importances\n","importances = clf.feature_importances_\n","\n","# Print feature importances\n","for i, importance in enumerate(importances):\n","    print(f\"Feature {i}: {iris.feature_names[i]}, Importance: {importance}\")"]},{"cell_type":"markdown","metadata":{"id":"QV0UgrAFA9AU"},"source":["Reasoning:\n","\n","Import necessary libraries: We import pandas, DecisionTreeClassifier, load_iris, and train_test_split.\n","Load the Iris dataset: Similar to the previous example, we load the dataset using load_iris().\n","Split the data: Again, we use train_test_split to divide the data into training and testing sets.\n","Create and train the model: We create a DecisionTreeClassifier object with criterion='gini' to specify Gini impurity as the criterion. Then, we train the model using fit().\n","Get and print feature importances: We access the feature importances using clf.feature_importances_ and print them along with the corresponding feature names."]},{"cell_type":"markdown","metadata":{"id":"oy_0VsKtBBBO"},"source":["18. **Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n","model accuracy ?**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1158,"status":"ok","timestamp":1747297037675,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"_C32bG7fBJIv","outputId":"9212809d-ad46-4539-8e2d-08e7ae74ad6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9777777777777777\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier object with Entropy as the criterion\n","clf = DecisionTreeClassifier(criterion='entropy')\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = clf.predict(X_test)\n","\n","# Calculate the accuracy of the model\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"Kf2JYMmsBPhN"},"source":["Reasoning\n","\n","Import necessary libraries: We import load_iris to load the dataset, train_test_split to split data, DecisionTreeClassifier for the model, and accuracy_score for evaluation.\n","Load the Iris dataset: load_iris() loads the dataset, and we assign features to X and the target variable to y.\n","Split the data: train_test_split divides the data into training and testing sets (70% for training, 30% for testing). random_state ensures consistent splitting.\n","Create and train the model: We create a DecisionTreeClassifier object with criterion='entropy' to specify Entropy as the criterion. Then, we train the model using fit().\n","Make predictions: We use the trained model to predict on the testing data (X_test).\n","Calculate and print accuracy: accuracy_score compares predictions (y_pred) with actual labels (y_test) to calculate accuracy, which is then printed."]},{"cell_type":"markdown","metadata":{"id":"p8_Jm4PjBUz_"},"source":["19. **Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n","Squared Error (MSE) ?**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":1942,"status":"error","timestamp":1747297120653,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"l-9Cd6QVBbMY","outputId":"257f71d1-c918-4201-d107-32dd76cc59e1"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'housing.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-4-70d96810388d\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the housing dataset (replace 'housing.csv' with your dataset file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'housing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Assuming 'target' is the target variable column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'housing.csv'"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the housing dataset (replace 'housing.csv' with your dataset file)\n","data = pd.read_csv('housing.csv')\n","\n","# Assuming 'target' is the target variable column name\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a DecisionTreeRegressor object\n","regressor = DecisionTreeRegressor()\n","\n","# Fit the model to the training data\n","regressor.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = regressor.predict(X_test)\n","\n","# Calculate the Mean Squared Error (MSE)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","# Print the MSE\n","print(\"Mean Squared Error (MSE):\", mse)"]},{"cell_type":"markdown","metadata":{"id":"K4KqU69IBkv-"},"source":["Reasoning:\n","\n","Import necessary libraries: We import pandas for data handling, DecisionTreeRegressor for the model, train_test_split for data splitting, and mean_squared_error for evaluation.\n","Load the housing dataset: Replace 'housing.csv' with the actual path to your housing dataset file. We use pd.read_csv to load the data into a pandas DataFrame.\n","Prepare the data: Assuming your target variable column is named 'target', we separate features (X) and the target (y) using data.drop and data['target'], respectively.\n","Split the data: train_test_split divides the data into training and testing sets (80% for training, 20% for testing). random_state ensures consistent splitting.\n","Create and train the model: We create a DecisionTreeRegressor object and train it using the training data (X_train, y_train).\n","Make predictions: We use the trained model to predict on the testing data (X_test).\n","Calculate and print MSE: mean_squared_error calculates the MSE between actual values (y_test) and predictions (y_pred), which is then printed.\n","Before running the code\n","\n","Replace 'housing.csv' with the actual path to your housing dataset file.\n","Ensure that the target variable column in your dataset is named 'target'. If it's named differently, adjust the code accordingly (e.g., y = data['house_price'])."]},{"cell_type":"markdown","metadata":{"id":"u88wlJXJBloG"},"source":["20. **Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz ?**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":936},"executionInfo":{"elapsed":1827,"status":"ok","timestamp":1747297346100,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"7oNYagXzBrjV","outputId":"c56ff179-d4ab-4d35-bcee-2eea198a3a23"},"outputs":[{"data":{"image/svg+xml":"\u003c?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?\u003e\n\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\n\u003c!-- Generated by graphviz version 2.43.0 (0)\n --\u003e\n\u003c!-- Title: Tree Pages: 1 --\u003e\n\u003csvg width=\"863pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 863.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\n\u003cg id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\"\u003e\n\u003ctitle\u003eTree\u003c/title\u003e\n\u003cpolygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 859,-667 859,4 -4,4\"/\u003e\n\u003c!-- 0 --\u003e\n\u003cg id=\"node1\" class=\"node\"\u003e\n\u003ctitle\u003e0\u003c/title\u003e\n\u003cpath fill=\"#ffffff\" stroke=\"black\" d=\"M509.5,-663C509.5,-663 387.5,-663 387.5,-663 381.5,-663 375.5,-657 375.5,-651 375.5,-651 375.5,-592 375.5,-592 375.5,-586 381.5,-580 387.5,-580 387.5,-580 509.5,-580 509.5,-580 515.5,-580 521.5,-586 521.5,-592 521.5,-592 521.5,-651 521.5,-651 521.5,-657 515.5,-663 509.5,-663\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"383.5\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal width (cm) ≤ 0.8\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"413\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.664\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"403.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 105\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"390.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [31, 37, 37]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"396\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 1 --\u003e\n\u003cg id=\"node2\" class=\"node\"\u003e\n\u003ctitle\u003e1\u003c/title\u003e\n\u003cpath fill=\"#e58139\" stroke=\"black\" d=\"M418,-536.5C418,-536.5 325,-536.5 325,-536.5 319,-536.5 313,-530.5 313,-524.5 313,-524.5 313,-480.5 313,-480.5 313,-474.5 319,-468.5 325,-468.5 325,-468.5 418,-468.5 418,-468.5 424,-468.5 430,-474.5 430,-480.5 430,-480.5 430,-524.5 430,-524.5 430,-530.5 424,-536.5 418,-536.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"343.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"330.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 31\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"321\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [31, 0, 0]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"328\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = setosa\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 0\u0026#45;\u0026gt;1 --\u003e\n\u003cg id=\"edge1\" class=\"edge\"\u003e\n\u003ctitle\u003e0\u0026#45;\u0026gt;1\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M421.79,-579.91C414.38,-568.65 406.33,-556.42 398.88,-545.11\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"401.75,-543.1 393.33,-536.67 395.9,-546.94 401.75,-543.1\"/\u003e\n\u003ctext text-anchor=\"middle\" x=\"388.28\" y=\"-557.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eTrue\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 2 --\u003e\n\u003cg id=\"node3\" class=\"node\"\u003e\n\u003ctitle\u003e2\u003c/title\u003e\n\u003cpath fill=\"#ffffff\" stroke=\"black\" d=\"M590.5,-544C590.5,-544 460.5,-544 460.5,-544 454.5,-544 448.5,-538 448.5,-532 448.5,-532 448.5,-473 448.5,-473 448.5,-467 454.5,-461 460.5,-461 460.5,-461 590.5,-461 590.5,-461 596.5,-461 602.5,-467 602.5,-473 602.5,-473 602.5,-532 602.5,-532 602.5,-538 596.5,-544 590.5,-544\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"456.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal width (cm) ≤ 1.75\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"497.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.5\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"484.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 74\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"471\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 37, 37]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"473\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 0\u0026#45;\u0026gt;2 --\u003e\n\u003cg id=\"edge2\" class=\"edge\"\u003e\n\u003ctitle\u003e0\u0026#45;\u0026gt;2\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M475.21,-579.91C481.01,-571.1 487.2,-561.7 493.18,-552.61\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"496.26,-554.3 498.83,-544.02 490.41,-550.45 496.26,-554.3\"/\u003e\n\u003ctext text-anchor=\"middle\" x=\"503.88\" y=\"-564.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eFalse\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 3 --\u003e\n\u003cg id=\"node4\" class=\"node\"\u003e\n\u003ctitle\u003e3\u003c/title\u003e\n\u003cpath fill=\"#54e992\" stroke=\"black\" d=\"M480,-425C480,-425 345,-425 345,-425 339,-425 333,-419 333,-413 333,-413 333,-354 333,-354 333,-348 339,-342 345,-342 345,-342 480,-342 480,-342 486,-342 492,-348 492,-354 492,-354 492,-413 492,-413 492,-419 486,-425 480,-425\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"341\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal length (cm) ≤ 4.95\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"377\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.214\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"371.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 41\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"362\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 36, 5]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"360\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 2\u0026#45;\u0026gt;3 --\u003e\n\u003cg id=\"edge3\" class=\"edge\"\u003e\n\u003ctitle\u003e2\u0026#45;\u0026gt;3\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M486.3,-460.91C477.44,-451.74 467.96,-441.93 458.85,-432.49\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"461.1,-429.78 451.64,-425.02 456.07,-434.65 461.1,-429.78\"/\u003e\n\u003c/g\u003e\n\u003c!-- 12 --\u003e\n\u003cg id=\"node13\" class=\"node\"\u003e\n\u003ctitle\u003e12\u003c/title\u003e\n\u003cpath fill=\"#853fe6\" stroke=\"black\" d=\"M706,-425C706,-425 571,-425 571,-425 565,-425 559,-419 559,-413 559,-413 559,-354 559,-354 559,-348 565,-342 571,-342 571,-342 706,-342 706,-342 712,-342 718,-348 718,-354 718,-354 718,-413 718,-413 718,-419 712,-425 706,-425\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"567\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal length (cm) ≤ 4.85\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"603\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.059\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"597.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 33\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"588\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 1, 32]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"590\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 2\u0026#45;\u0026gt;12 --\u003e\n\u003cg id=\"edge12\" class=\"edge\"\u003e\n\u003ctitle\u003e2\u0026#45;\u0026gt;12\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M564.7,-460.91C573.56,-451.74 583.04,-441.93 592.15,-432.49\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"594.93,-434.65 599.36,-425.02 589.9,-429.78 594.93,-434.65\"/\u003e\n\u003c/g\u003e\n\u003c!-- 4 --\u003e\n\u003cg id=\"node5\" class=\"node\"\u003e\n\u003ctitle\u003e4\u003c/title\u003e\n\u003cpath fill=\"#3fe685\" stroke=\"black\" d=\"M256.5,-306C256.5,-306 134.5,-306 134.5,-306 128.5,-306 122.5,-300 122.5,-294 122.5,-294 122.5,-235 122.5,-235 122.5,-229 128.5,-223 134.5,-223 134.5,-223 256.5,-223 256.5,-223 262.5,-223 268.5,-229 268.5,-235 268.5,-235 268.5,-294 268.5,-294 268.5,-300 262.5,-306 256.5,-306\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"130.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal width (cm) ≤ 1.6\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"160\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.056\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"154.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 35\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"145\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 34, 1]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"143\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 3\u0026#45;\u0026gt;4 --\u003e\n\u003cg id=\"edge4\" class=\"edge\"\u003e\n\u003ctitle\u003e3\u0026#45;\u0026gt;4\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M337.21,-341.91C318,-331.55 297.25,-320.36 277.71,-309.82\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"279.29,-306.7 268.82,-305.03 275.96,-312.86 279.29,-306.7\"/\u003e\n\u003c/g\u003e\n\u003c!-- 7 --\u003e\n\u003cg id=\"node8\" class=\"node\"\u003e\n\u003ctitle\u003e7\u003c/title\u003e\n\u003cpath fill=\"#c09cf2\" stroke=\"black\" d=\"M477.5,-306C477.5,-306 347.5,-306 347.5,-306 341.5,-306 335.5,-300 335.5,-294 335.5,-294 335.5,-235 335.5,-235 335.5,-229 341.5,-223 347.5,-223 347.5,-223 477.5,-223 477.5,-223 483.5,-223 489.5,-229 489.5,-235 489.5,-235 489.5,-294 489.5,-294 489.5,-300 483.5,-306 477.5,-306\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"343.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal width (cm) ≤ 1.55\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"377\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.444\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"375\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 6\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"365.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 2, 4]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"364\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 3\u0026#45;\u0026gt;7 --\u003e\n\u003cg id=\"edge7\" class=\"edge\"\u003e\n\u003ctitle\u003e3\u0026#45;\u0026gt;7\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M412.5,-341.91C412.5,-333.65 412.5,-324.86 412.5,-316.3\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"416,-316.02 412.5,-306.02 409,-316.02 416,-316.02\"/\u003e\n\u003c/g\u003e\n\u003c!-- 5 --\u003e\n\u003cg id=\"node6\" class=\"node\"\u003e\n\u003ctitle\u003e5\u003c/title\u003e\n\u003cpath fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"32.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 34\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 34, 0]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 4\u0026#45;\u0026gt;5 --\u003e\n\u003cg id=\"edge5\" class=\"edge\"\u003e\n\u003ctitle\u003e4\u0026#45;\u0026gt;5\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M148.66,-222.91C135.04,-211.1 120.17,-198.22 106.6,-186.45\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"108.62,-183.57 98.77,-179.67 104.03,-188.86 108.62,-183.57\"/\u003e\n\u003c/g\u003e\n\u003c!-- 6 --\u003e\n\u003cg id=\"node7\" class=\"node\"\u003e\n\u003ctitle\u003e6\u003c/title\u003e\n\u003cpath fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"167.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"158\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 1\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"148.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 0, 1]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"147\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 4\u0026#45;\u0026gt;6 --\u003e\n\u003cg id=\"edge6\" class=\"edge\"\u003e\n\u003ctitle\u003e4\u0026#45;\u0026gt;6\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M195.5,-222.91C195.5,-212.2 195.5,-200.62 195.5,-189.78\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"199,-189.67 195.5,-179.67 192,-189.67 199,-189.67\"/\u003e\n\u003c/g\u003e\n\u003c!-- 8 --\u003e\n\u003cg id=\"node9\" class=\"node\"\u003e\n\u003ctitle\u003e8\u003c/title\u003e\n\u003cpath fill=\"#8139e5\" stroke=\"black\" d=\"M371,-179.5C371,-179.5 282,-179.5 282,-179.5 276,-179.5 270,-173.5 270,-167.5 270,-167.5 270,-123.5 270,-123.5 270,-117.5 276,-111.5 282,-111.5 282,-111.5 371,-111.5 371,-111.5 377,-111.5 383,-117.5 383,-123.5 383,-123.5 383,-167.5 383,-167.5 383,-173.5 377,-179.5 371,-179.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"298.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"289\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 3\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"279.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 0, 3]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"278\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 7\u0026#45;\u0026gt;8 --\u003e\n\u003cg id=\"edge8\" class=\"edge\"\u003e\n\u003ctitle\u003e7\u0026#45;\u0026gt;8\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M382.66,-222.91C374.31,-211.54 365.22,-199.18 356.84,-187.77\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"359.62,-185.65 350.88,-179.67 353.98,-189.8 359.62,-185.65\"/\u003e\n\u003c/g\u003e\n\u003c!-- 9 --\u003e\n\u003cg id=\"node10\" class=\"node\"\u003e\n\u003ctitle\u003e9\u003c/title\u003e\n\u003cpath fill=\"#9cf2c0\" stroke=\"black\" d=\"M548,-187C548,-187 413,-187 413,-187 407,-187 401,-181 401,-175 401,-175 401,-116 401,-116 401,-110 407,-104 413,-104 413,-104 548,-104 548,-104 554,-104 560,-110 560,-116 560,-116 560,-175 560,-175 560,-181 554,-187 548,-187\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"409\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003epetal length (cm) ≤ 5.45\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"445\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.444\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"443\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 3\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"433.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 2, 1]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 7\u0026#45;\u0026gt;9 --\u003e\n\u003cg id=\"edge9\" class=\"edge\"\u003e\n\u003ctitle\u003e7\u0026#45;\u0026gt;9\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M436.09,-222.91C441.16,-214.2 446.56,-204.9 451.79,-195.89\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"454.95,-197.43 456.95,-187.02 448.9,-193.91 454.95,-197.43\"/\u003e\n\u003c/g\u003e\n\u003c!-- 10 --\u003e\n\u003cg id=\"node11\" class=\"node\"\u003e\n\u003ctitle\u003e10\u003c/title\u003e\n\u003cpath fill=\"#39e581\" stroke=\"black\" d=\"M461,-68C461,-68 364,-68 364,-68 358,-68 352,-62 352,-56 352,-56 352,-12 352,-12 352,-6 358,0 364,0 364,0 461,0 461,0 467,0 473,-6 473,-12 473,-12 473,-56 473,-56 473,-62 467,-68 461,-68\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"384.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"375\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 2\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"365.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 2, 0]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"360\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 9\u0026#45;\u0026gt;10 --\u003e\n\u003cg id=\"edge10\" class=\"edge\"\u003e\n\u003ctitle\u003e9\u0026#45;\u0026gt;10\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M455.18,-103.73C449.74,-94.97 443.99,-85.7 438.52,-76.91\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"441.43,-74.95 433.18,-68.3 435.48,-78.64 441.43,-74.95\"/\u003e\n\u003c/g\u003e\n\u003c!-- 11 --\u003e\n\u003cg id=\"node12\" class=\"node\"\u003e\n\u003ctitle\u003e11\u003c/title\u003e\n\u003cpath fill=\"#8139e5\" stroke=\"black\" d=\"M592,-68C592,-68 503,-68 503,-68 497,-68 491,-62 491,-56 491,-56 491,-12 491,-12 491,-6 497,0 503,0 503,0 592,0 592,0 598,0 604,-6 604,-12 604,-12 604,-56 604,-56 604,-62 598,-68 592,-68\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"519.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"510\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 1\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"500.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 0, 1]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 9\u0026#45;\u0026gt;11 --\u003e\n\u003cg id=\"edge11\" class=\"edge\"\u003e\n\u003ctitle\u003e9\u0026#45;\u0026gt;11\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M505.45,-103.73C510.81,-94.97 516.48,-85.7 521.86,-76.91\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"524.89,-78.66 527.12,-68.3 518.92,-75 524.89,-78.66\"/\u003e\n\u003c/g\u003e\n\u003c!-- 13 --\u003e\n\u003cg id=\"node14\" class=\"node\"\u003e\n\u003ctitle\u003e13\u003c/title\u003e\n\u003cpath fill=\"#c09cf2\" stroke=\"black\" d=\"M707.5,-306C707.5,-306 569.5,-306 569.5,-306 563.5,-306 557.5,-300 557.5,-294 557.5,-294 557.5,-235 557.5,-235 557.5,-229 563.5,-223 569.5,-223 569.5,-223 707.5,-223 707.5,-223 713.5,-223 719.5,-229 719.5,-235 719.5,-235 719.5,-294 719.5,-294 719.5,-300 713.5,-306 707.5,-306\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"565.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esepal length (cm) ≤ 5.95\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"603\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.444\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"601\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 3\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"591.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 1, 2]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"590\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 12\u0026#45;\u0026gt;13 --\u003e\n\u003cg id=\"edge13\" class=\"edge\"\u003e\n\u003ctitle\u003e12\u0026#45;\u0026gt;13\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M638.5,-341.91C638.5,-333.65 638.5,-324.86 638.5,-316.3\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"642,-316.02 638.5,-306.02 635,-316.02 642,-316.02\"/\u003e\n\u003c/g\u003e\n\u003c!-- 16 --\u003e\n\u003cg id=\"node17\" class=\"node\"\u003e\n\u003ctitle\u003e16\u003c/title\u003e\n\u003cpath fill=\"#8139e5\" stroke=\"black\" d=\"M843,-298.5C843,-298.5 750,-298.5 750,-298.5 744,-298.5 738,-292.5 738,-286.5 738,-286.5 738,-242.5 738,-242.5 738,-236.5 744,-230.5 750,-230.5 750,-230.5 843,-230.5 843,-230.5 849,-230.5 855,-236.5 855,-242.5 855,-242.5 855,-286.5 855,-286.5 855,-292.5 849,-298.5 843,-298.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"768.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"755.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 30\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"746\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 0, 30]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"748\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 12\u0026#45;\u0026gt;16 --\u003e\n\u003cg id=\"edge16\" class=\"edge\"\u003e\n\u003ctitle\u003e12\u0026#45;\u0026gt;16\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M693.32,-341.91C709.56,-329.88 727.31,-316.73 743.44,-304.79\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"745.76,-307.43 751.71,-298.67 741.59,-301.81 745.76,-307.43\"/\u003e\n\u003c/g\u003e\n\u003c!-- 14 --\u003e\n\u003cg id=\"node15\" class=\"node\"\u003e\n\u003ctitle\u003e14\u003c/title\u003e\n\u003cpath fill=\"#39e581\" stroke=\"black\" d=\"M687,-179.5C687,-179.5 590,-179.5 590,-179.5 584,-179.5 578,-173.5 578,-167.5 578,-167.5 578,-123.5 578,-123.5 578,-117.5 584,-111.5 590,-111.5 590,-111.5 687,-111.5 687,-111.5 693,-111.5 699,-117.5 699,-123.5 699,-123.5 699,-167.5 699,-167.5 699,-173.5 693,-179.5 687,-179.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"610.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"601\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 1\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"591.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 1, 0]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"586\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = versicolor\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 13\u0026#45;\u0026gt;14 --\u003e\n\u003cg id=\"edge14\" class=\"edge\"\u003e\n\u003ctitle\u003e13\u0026#45;\u0026gt;14\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M638.5,-222.91C638.5,-212.2 638.5,-200.62 638.5,-189.78\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"642,-189.67 638.5,-179.67 635,-189.67 642,-189.67\"/\u003e\n\u003c/g\u003e\n\u003c!-- 15 --\u003e\n\u003cg id=\"node16\" class=\"node\"\u003e\n\u003ctitle\u003e15\u003c/title\u003e\n\u003cpath fill=\"#8139e5\" stroke=\"black\" d=\"M818,-179.5C818,-179.5 729,-179.5 729,-179.5 723,-179.5 717,-173.5 717,-167.5 717,-167.5 717,-123.5 717,-123.5 717,-117.5 723,-111.5 729,-111.5 729,-111.5 818,-111.5 818,-111.5 824,-111.5 830,-117.5 830,-123.5 830,-123.5 830,-167.5 830,-167.5 830,-173.5 824,-179.5 818,-179.5\"/\u003e\n\u003ctext text-anchor=\"start\" x=\"745.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003egini = 0.0\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"736\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003esamples = 2\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"726.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003evalue = [0, 0, 2]\u003c/text\u003e\n\u003ctext text-anchor=\"start\" x=\"725\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"\u003eclass = virginica\u003c/text\u003e\n\u003c/g\u003e\n\u003c!-- 13\u0026#45;\u0026gt;15 --\u003e\n\u003cg id=\"edge15\" class=\"edge\"\u003e\n\u003ctitle\u003e13\u0026#45;\u0026gt;15\u003c/title\u003e\n\u003cpath fill=\"none\" stroke=\"black\" d=\"M685.34,-222.91C698.96,-211.1 713.83,-198.22 727.4,-186.45\"/\u003e\n\u003cpolygon fill=\"black\" stroke=\"black\" points=\"729.97,-188.86 735.23,-179.67 725.38,-183.57 729.97,-188.86\"/\u003e\n\u003c/g\u003e\n\u003c/g\u003e\n\u003c/svg\u003e\n","text/plain":["\u003cgraphviz.sources.Source at 0x7db3f8c5fb10\u003e"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","import graphviz\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier object\n","clf = DecisionTreeClassifier()\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Export the decision tree to a dot file\n","export_graphviz(clf, out_file=\"tree.dot\",\n","                feature_names=iris.feature_names,\n","                class_names=iris.target_names,\n","                filled=True, rounded=True,\n","                special_characters=True)\n","\n","# Visualize the tree using graphviz\n","with open(\"tree.dot\") as f:\n","    dot_graph = f.read()\n","graphviz.Source(dot_graph)"]},{"cell_type":"markdown","metadata":{"id":"aykXZmD0CKJN"},"source":["Reasoning:\n","\n","Import necessary libraries: We import load_iris for the dataset, train_test_split for data splitting, DecisionTreeClassifier and export_graphviz for the model and visualization, and graphviz for rendering.\n","Load and split the data: Similar to previous examples, we load the Iris dataset and split it into training and testing sets.\n","Create and train the model: We create a DecisionTreeClassifier object and train it using the training data.\n","Export the tree: export_graphviz converts the trained tree into a dot file (tree.dot). We provide feature names, class names, and formatting options for better visualization.\n","Visualize the tree: We use graphviz.Source to read the dot file and render the tree visualization within your Colab environment.\n","Before running the code:\n","\n","Install graphviz: If you haven't already, install the graphviz library in your Colab environment:\n","!pip install graphviz\n","Use code with caution\n","Install system dependencies: You might also need to install system dependencies for graphviz. Run the following command in your Colab cell:\n","!apt-get install graphviz\n","Use code with caution\n","After running the code, you should see the decision tree visualization displayed in your Colab output."]},{"cell_type":"markdown","metadata":{"id":"6mboI9rBCOz2"},"source":["21. **Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n","accuracy with a fully grown tree ?**"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2568,"status":"ok","timestamp":1747297412876,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"qr0_JSHECl1X","outputId":"5b45be70-e9c0-4134-a2f8-ad2dd1ca0ddb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (Limited Depth): 1.0\n","Accuracy (Full Tree): 1.0\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier with maximum depth of 3\n","clf_limited = DecisionTreeClassifier(max_depth=3)\n","\n","# Create a fully grown Decision Tree Classifier\n","clf_full = DecisionTreeClassifier()\n","\n","# Train the classifiers\n","clf_limited.fit(X_train, y_train)\n","clf_full.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_limited = clf_limited.predict(X_test)\n","y_pred_full = clf_full.predict(X_test)\n","\n","# Calculate accuracy scores\n","accuracy_limited = accuracy_score(y_test, y_pred_limited)\n","accuracy_full = accuracy_score(y_test, y_pred_full)\n","\n","# Print the accuracy scores\n","print(\"Accuracy (Limited Depth):\", accuracy_limited)\n","print(\"Accuracy (Full Tree):\", accuracy_full)"]},{"cell_type":"markdown","metadata":{"id":"B-D92FfzCrxL"},"source":["Reasoning\n","\n","Import necessary libraries: We import load_iris for the dataset, train_test_split for data splitting, DecisionTreeClassifier for the model, and accuracy_score for evaluation.\n","Load and split the data: Similar to previous examples, we load the Iris dataset and split it into training and testing sets.\n","Create two classifiers: We create two DecisionTreeClassifier objects:\n","clf_limited: With max_depth=3 to restrict the tree's depth.\n","clf_full: Without any depth restriction, allowing it to grow fully.\n","Train the classifiers: We train both classifiers using the training data.\n","Make predictions: We make predictions on the testing data using both classifiers.\n","Calculate and print accuracy: We calculate the accuracy scores for both classifiers using accuracy_score and print them for comparison.\n","By comparing the accuracy scores, you can observe the effect of limiting the tree's depth on its performance. In some cases, limiting the depth can help prevent overfitting and improve generalization to unseen data. In other cases, a fully grown tree might achieve better accuracy on the training data but could potentially overfit."]},{"cell_type":"markdown","metadata":{"id":"AcLKTvPICtFN"},"source":["22. **Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n","accuracy with a default tree ?**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2113,"status":"ok","timestamp":1747297529104,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"N35CBKxWCydb","outputId":"a596577e-4373-4adc-da93-8de92b615416"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (min_samples_split=5): 1.0\n","Accuracy (Default): 1.0\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier with min_samples_split=5\n","clf_min_split = DecisionTreeClassifier(min_samples_split=5)\n","\n","# Create a default Decision Tree Classifier\n","clf_default = DecisionTreeClassifier()\n","\n","# Train the classifiers\n","clf_min_split.fit(X_train, y_train)\n","clf_default.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_min_split = clf_min_split.predict(X_test)\n","y_pred_default = clf_default.predict(X_test)\n","\n","# Calculate accuracy scores\n","accuracy_min_split = accuracy_score(y_test, y_pred_min_split)\n","accuracy_default = accuracy_score(y_test, y_pred_default)\n","\n","# Print the accuracy scores\n","print(\"Accuracy (min_samples_split=5):\", accuracy_min_split)\n","print(\"Accuracy (Default):\", accuracy_default)"]},{"cell_type":"markdown","metadata":{"id":"CQfUxMN4DJOY"},"source":["Reasoning:\n","\n","Import necessary libraries: We import the usual libraries for loading the dataset, splitting data, creating the model, and evaluating accuracy.\n","Load and split the data: We load the Iris dataset and split it into training and testing sets.\n","Create two classifiers:\n","clf_min_split: This classifier is created with min_samples_split=5, meaning a node will only be split if it has at least 5 samples.\n","clf_default: This classifier is created with default parameters, which means it will use the default value for min_samples_split (usually 2).\n","Train the classifiers: Both classifiers are trained using the training data.\n","Make predictions: Predictions are made on the testing data using both classifiers.\n","Calculate and print accuracy: The accuracy scores for both classifiers are calculated and printed to compare their performance.\n","By comparing the accuracy scores, you can observe the effect of changing the min_samples_split parameter on the model's performance. Increasing min_samples_split can sometimes help prevent overfitting, as it makes the tree less likely to create very specific splits based on small groups of data points. However, it can also lead to underfitting if the value is too high."]},{"cell_type":"markdown","metadata":{"id":"Sd9F9W-5DKgK"},"source":["23. **Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n","accuracy with unscaled data ?**"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1373,"status":"ok","timestamp":1747297610218,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"TlzRb-D-DRj2","outputId":"aad8f85b-31e5-436f-c292-afe7eba1e424"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (Scaled Data): 1.0\n","Accuracy (Unscaled Data): 1.0\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a StandardScaler object\n","scaler = StandardScaler()\n","\n","# Fit the scaler on the training data and transform\n","X_train_scaled = scaler.fit_transform(X_train)\n","\n","# Transform the testing data using the fitted scaler\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Create and train a Decision Tree Classifier with scaled data\n","clf_scaled = DecisionTreeClassifier()\n","clf_scaled.fit(X_train_scaled, y_train)\n","\n","# Create and train a Decision Tree Classifier with unscaled data\n","clf_unscaled = DecisionTreeClassifier()\n","clf_unscaled.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_scaled = clf_scaled.predict(X_test_scaled)\n","y_pred_unscaled = clf_unscaled.predict(X_test)\n","\n","# Calculate accuracy scores\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n","\n","# Print the accuracy scores\n","print(\"Accuracy (Scaled Data):\", accuracy_scaled)\n","print(\"Accuracy (Unscaled Data):\", accuracy_unscaled)"]},{"cell_type":"markdown","metadata":{"id":"2exBTOLiDdD5"},"source":["Reasoning:\n","\n","Import necessary libraries: We import load_iris, train_test_split, DecisionTreeClassifier, StandardScaler, and accuracy_score.\n","Load and split the data: We load the Iris dataset and split it into training and testing sets.\n","Feature scaling:\n","Create a StandardScaler object to perform standardization.\n","Fit the scaler on the training data using fit_transform to learn the scaling parameters.\n","Transform both the training and testing data using the fitted scaler.\n","Create and train classifiers:\n","clf_scaled: Trained on the scaled data.\n","clf_unscaled: Trained on the original, unscaled data.\n","Make predictions: Predictions are made on the appropriately scaled testing data for each classifier.\n","Calculate and print accuracy: The accuracy scores for both classifiers are calculated and printed to compare their performance.\n","Important considerations\n","\n","Decision Trees and Feature Scaling: Decision Trees are generally not very sensitive to feature scaling. This is because they make decisions based on thresholds on individual features, and scaling doesn't change the relative order of values within a feature.\n","Potential Impact: In some cases, feature scaling might slightly improve or worsen the accuracy of a Decision Tree Classifier, but the difference is usually not significant.\n","Experimentation: It's always a good idea to experiment with both scaled and unscaled data to see if there's any noticeable impact on your specific dataset and task."]},{"cell_type":"markdown","metadata":{"id":"0enYFNG_DeVI"},"source":["24. **Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n","classification ?**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1193,"status":"ok","timestamp":1747297681256,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"7u_IrcbZDkJv","outputId":"fa4c6eed-2d1c-4991-db20-3cb817b189f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 1.0\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier as the base estimator\n","base_estimator = DecisionTreeClassifier()\n","\n","# Create an OvR classifier using the Decision Tree as the base estimator\n","ovr_classifier = OneVsRestClassifier(base_estimator)\n","\n","# Train the OvR classifier on the training data\n","ovr_classifier.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = ovr_classifier.predict(X_test)\n","\n","# Calculate the accuracy of the model\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"CJvscvDLD8IE"},"source":["Reasoning:\n","\n","Import necessary libraries: We import load_iris for the dataset, train_test_split for data splitting, DecisionTreeClassifier for the base model, OneVsRestClassifier for the OvR strategy, and accuracy_score for evaluation.\n","Load and split the data: We load the Iris dataset, which is a multi-class classification problem, and split it into training and testing sets.\n","Create the base estimator: We create a DecisionTreeClassifier object as the base estimator, which will be used by the OvR classifier.\n","Create the OvR classifier: We create an OneVsRestClassifier object, passing the base_estimator as an argument. This sets up the OvR strategy using the Decision Tree as the underlying model.\n","Train the OvR classifier: We train the ovr_classifier using the training data (X_train, y_train).\n","Make predictions: We make predictions on the testing data (X_test) using the trained OvR classifier.\n","Calculate and print accuracy: We calculate the accuracy of the model using accuracy_score and print it to evaluate its performance.\n","How OvR works\n","\n","Binary Classifiers: OvR creates a separate binary classifier for each class in the dataset.\n","Training: Each classifier is trained to distinguish between one class (the \"positive\" class) and all other classes (the \"negative\" class).\n","Prediction: When making a prediction for a new data point, the OvR classifier runs all the binary classifiers and assigns the data point to the class with the highest predicted probability or confidence.\n","This approach allows Decision Trees, which are inherently binary classifiers, to be used for multi-class classification problems."]},{"cell_type":"markdown","metadata":{"id":"scERZ03kD_hH"},"source":["25. **Write a Python program to train a Decision Tree Classifier and display the feature importance scores ?**"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1747297845595,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"yJZ6SLRnEFIb","outputId":"835eafb5-124a-46dd-b1e1-dad427bd66c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["             Feature  Importance\n","2  petal length (cm)    0.557274\n","3   petal width (cm)    0.423616\n","0  sepal length (cm)    0.019110\n","1   sepal width (cm)    0.000000\n"]}],"source":["# Import necessary libraries\n","import pandas as pd  # For creating DataFrames for better display\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier object\n","clf = DecisionTreeClassifier()\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Get feature importances\n","importances = clf.feature_importances_\n","\n","# Create a DataFrame for better display\n","feature_importances = pd.DataFrame({'Feature': iris.feature_names, 'Importance': importances})\n","\n","# Sort the DataFrame by importance in descending order\n","feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n","\n","# Display the feature importances\n","print(feature_importances)"]},{"cell_type":"markdown","metadata":{"id":"kjejHyjBEWe6"},"source":["Reasoning\n","\n","Import necessary libraries: We import pandas for creating DataFrames, load_iris for the dataset, train_test_split for data splitting, and DecisionTreeClassifier for the model.\n","Load and split the data: We load the Iris dataset and split it into training and testing sets.\n","Create and train the classifier: We create a DecisionTreeClassifier object and train it using the training data.\n","Get feature importances: We access the feature importance scores using clf.feature_importances_.\n","Create a DataFrame: We create a pandas DataFrame to store the feature names and their corresponding importance scores for better display.\n","Sort the DataFrame: We sort the DataFrame in descending order of importance to easily see the most important features.\n","Display the feature importances: We print the DataFrame to display the feature importance scores.\n","Understanding Feature Importance\n","\n","Gini Importance/Mean Decrease Impurity: The feature importance scores in a Decision Tree are calculated based on how much each feature reduces the impurity (Gini impurity or entropy) of the nodes in the tree.\n","Higher Scores: Features with higher importance scores contribute more to the decision-making process of the tree and are considered more important for predicting the target variable."]},{"cell_type":"markdown","metadata":{"id":"-hNH22KqEaO6"},"source":["26. **Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n","with an unrestricted tree ?**"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":681,"status":"error","timestamp":1747297937509,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"9PspYrxyEg9L","outputId":"d827201a-835d-48e6-8d54-2830617b59e2"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'housing.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-2109daa33cc4\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the dataset (replace 'housing.csv' with your dataset file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'housing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Assuming 'target' is the target variable column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'housing.csv'"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the dataset (replace 'housing.csv' with your dataset file)\n","data = pd.read_csv('housing.csv')\n","\n","# Assuming 'target' is the target variable column name\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a Decision Tree Regressor with max_depth=5\n","regressor_limited = DecisionTreeRegressor(max_depth=5)\n","\n","# Create an unrestricted Decision Tree Regressor\n","regressor_unrestricted = DecisionTreeRegressor()\n","\n","# Train the regressors\n","regressor_limited.fit(X_train, y_train)\n","regressor_unrestricted.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_limited = regressor_limited.predict(X_test)\n","y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n","\n","# Calculate Mean Squared Error (MSE)\n","mse_limited = mean_squared_error(y_test, y_pred_limited)\n","mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n","\n","# Print the MSE scores\n","print(\"MSE (Limited Depth):\", mse_limited)\n","print(\"MSE (Unrestricted):\", mse_unrestricted)"]},{"cell_type":"markdown","metadata":{"id":"A9GOBJvwErYw"},"source":["Reasoning\n","\n","Import necessary libraries: We import pandas for data handling, DecisionTreeRegressor for the model, train_test_split for data splitting, and mean_squared_error for evaluation.\n","Load the dataset: Replace 'housing.csv' with the actual path to your housing dataset file. We use pd.read_csv to load the data into a pandas DataFrame.\n","Prepare the data: Assuming your target variable column is named 'target', we separate features (X) and the target (y) using data.drop and data['target'], respectively.\n","Split the data: train_test_split divides the data into training and testing sets (80% for training, 20% for testing). random_state ensures consistent splitting.\n","Create and train regressors:\n","regressor_limited: This regressor is created with max_depth=5, restricting the tree's depth.\n","regressor_unrestricted: This regressor is created without any depth restriction, allowing it to grow fully.\n","Make predictions: We make predictions on the testing data using both regressors.\n","Calculate and print MSE: We calculate the Mean Squared Error (MSE) for both regressors using mean_squared_error and print them for comparison.\n","Before running the code\n","\n","Replace 'housing.csv' with the actual path to your housing dataset file.\n","Ensure that the target variable column in your dataset is named 'target'. If it's named differently, adjust the code accordingly (e.g., y = data['house_price']).\n","By comparing the MSE scores, you can observe the effect of limiting the tree's depth on its performance. In regression tasks, limiting the depth can help prevent overfitting and improve generalization to unseen data, often resulting in a lower MSE on the testing set. However, it can also lead to underfitting if the value is too low."]},{"cell_type":"markdown","metadata":{"id":"qbzdIXKLEvw2"},"source":["27. **Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n","visualize its effect on accuracy ?**"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":2266,"status":"ok","timestamp":1747298039647,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"Bx4r218FE_XQ","outputId":"266ef34e-ba84-43ed-f794-2e0b1113ce59"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 1000x600 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier object\n","clf = DecisionTreeClassifier(random_state=0)\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Get the effective alphas and impurities from the trained tree\n","path = clf.cost_complexity_pruning_path(X_train, y_train)\n","ccp_alphas, impurities = path.ccp_alphas, path.impurities\n","\n","# Create a list to store the accuracy scores for different alphas\n","acc_scores = []\n","\n","# Iterate over different values of alpha\n","for ccp_alpha in ccp_alphas:\n","    # Create a new Decision Tree Classifier with the current alpha\n","    clf_ccp = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n","\n","    # Train the classifier on the training data\n","    clf_ccp.fit(X_train, y_train)\n","\n","    # Make predictions on the testing data\n","    y_pred = clf_ccp.predict(X_test)\n","\n","    # Calculate and store the accuracy score\n","    acc_scores.append(accuracy_score(y_test, y_pred))\n","\n","# Plot the accuracy scores vs. alphas\n","plt.figure(figsize=(10, 6))\n","plt.plot(ccp_alphas, acc_scores, marker='o', drawstyle=\"steps-post\")\n","plt.xlabel(\"alpha\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy vs alpha for training and testing sets\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bKVBmmv6FEcv"},"source":["Reasoning:\n","\n","Import Libraries: Import necessary libraries including matplotlib.pyplot for plotting.\n","Load and Split Data: Load the Iris dataset and split it into training and testing sets.\n","Create and Train Initial Classifier: Create a DecisionTreeClassifier and train it on the training data.\n","Get CCP Path: Use cost_complexity_pruning_path to get the effective alphas and impurities for pruning.\n","Iterate and Evaluate: Loop through different alpha values, create and train a new classifier with each alpha, make predictions, and store the accuracy scores.\n","Visualize: Plot the accuracy scores against the alpha values using matplotlib.pyplot. This shows how accuracy changes as pruning is applied.\n","How CCP Works:\n","\n","Cost Complexity: CCP considers the complexity of the tree (number of nodes) and the error it makes on the training data.\n","Alpha: The ccp_alpha parameter controls the amount of pruning. Higher alpha values lead to more aggressive pruning.\n","Finding Optimal Alpha: The plot helps you visualize the relationship between alpha and accuracy, allowing you to choose an alpha value that balances complexity and performance."]},{"cell_type":"markdown","metadata":{"id":"nCf54eOiFIVN"},"source":["28. **Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n","Recall, and F1-Score ?**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2018,"status":"ok","timestamp":1747298193854,"user":{"displayName":"Aryaveer Choudhary","userId":"10393937371124319623"},"user_tz":-330},"id":"VAm2-k4dFOm6","outputId":"c075559b-a1f1-4edf-ccc8-9f0b63bd7750"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 1.0\n","Precision: 1.0\n","Recall: 1.0\n","F1-Score: 1.0\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        19\n","           1       1.00      1.00      1.00        13\n","           2       1.00      1.00      1.00        13\n","\n","    accuracy                           1.00        45\n","   macro avg       1.00      1.00      1.00        45\n","weighted avg       1.00      1.00      1.00        45\n","\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Decision Tree Classifier object\n","clf = DecisionTreeClassifier()\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = clf.predict(X_test)\n","\n","# Calculate and print the evaluation metrics\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))  # Use 'weighted' for multiclass\n","print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))  # Use 'weighted' for multiclass\n","print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))  # Use 'weighted' for multiclass\n","\n","# You can also use classification_report for a more detailed report\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"2JoeAzywFgr8"},"source":["Reasoning:\n","\n","Import Libraries: Import necessary libraries including precision_score, recall_score, f1_score, and classification_report.\n","Load and Split Data: Load the Iris dataset and split it into training and testing sets.\n","Create and Train Classifier: Create a DecisionTreeClassifier and train it on the training data.\n","Make Predictions: Make predictions on the testing data using the trained classifier.\n","Calculate Metrics:\n","Use precision_score, recall_score, and f1_score with average='weighted' for multi-class problems to calculate the respective metrics.\n","Use classification_report to get a detailed report including precision, recall, F1-score, and support for each class.\n","Print Results: Print the calculated metrics.\n","Understanding the Metrics:\n","\n","Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n","Recall: The ratio of correctly predicted positive observations to all actual positives.\n","F1-Score: The harmonic mean of precision and recall, providing a balance between the two.\n","Classification Report: Provides a comprehensive overview of the model's performance for each class."]},{"cell_type":"markdown","metadata":{"id":"I2LXuvWiFhmH"},"source":["29.  **Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn ?**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LTDfOkViFo08"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 2 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMFQCTMzAl8YOHGYZgwNaxz","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}